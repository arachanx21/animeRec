# -*- coding: utf-8 -*-
"""animerec-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rNS1byl8jbyDLPStFAocEdiwLCqLLSjr
"""

# #run locally
# import pandas as pd
# import numpy as np
# import os
# pd.set_option("max_colwidth", None)
# # pd.set_option("display.max_columns",None)

# anime = pd.read_csv("anime.csv")
# users = pd.read_csv("rating.csv")
# path="./"

"""<h2>Data Loading</h2>"""

#run on kaggle or collab environment
import kagglehub, os
import pandas as pd
import numpy as np
pd.set_option("max_colwidth", None)
# pd.set_option("display.max_columns",None)

# Download latest version
path = kagglehub.dataset_download("CooperUnion/anime-recommendations-database")

print("Path to dataset files:", path)
os.listdir(path)

anime = pd.read_csv(os.path.join(path,"anime.csv"))
users = pd.read_csv(os.path.join(path,"rating.csv"))

"""<h2>Data Understanding</h2>"""

anime.info()

"""Dataset anime terdiri atas 7 variabel. 3 variabel merupakan variabel numerik (anime_id dan member memiliki tipe data int64 sedangkan rating memiliki tipedata float), sedangkan sisanya merupakan variabel kategorikal.

Menghitung jumlah data unik dalam dataset anime
"""

anime.nunique()

anime.head()

"""Data dalam variabel genre, nilai data merupakan sebuah list. Hal ini akan membuat ambiguitas dalam pemodelan nantinya. Data-data ini perlu di-encode agar proses cosine similarity dapat berjalan dengan baik. Namun, sebelum hal itu dilakukan, dataset perlu dicek terlebih dahulu apakah terdapat data yang tidak memiliki nilai.

terdapat data kosong dalam variable fitur type
"""

anime.isnull().sum()

"""terdapat 62 data kosong dalam variabel genre, 25 dalam variabel type dan 230  di rating.

<h3>Membersihkan data yang tidak memiliki nilai</h3>
"""

anime=anime.dropna()
anime.isnull().sum()

"""Setelah data-data yang tidak memiliki nilai dihapus, dataset sekarang tidak memiliki nilai yang kosong. Dataset diurutkan berdasarkan anime_id."""

anime=anime.sort_values("anime_id")
anime

anime.shape

"""Data yang telah dibersihkan sekarang memiliki 12017 baris data.

Memeriksa apakah data memiliki data duplikat
"""

anime=anime.drop_duplicates()
anime

"""Dataset tetap memiliki 12017 baris data. Data tidak memiliki nilai-nilai duplikat.

<h3>Vektorisasi Genre</h3>
<p>Agar data dapat dilihat kemiripannya, perlu dilakukan cosine similarity yang berupa vektor. Namun karena data pada genre masih dalam bentuk list pada nilai datanya, hal ini akan menimbulkan ambiguitas dan model akan kesulitan untuk mengidentifikasi kemiripannya. Oleh karena itu nilai-nilai pada list perlu dipecah terlebih dahulu ke satu array/list yang terdiri atas data genre satuan. Setelah itu, dicocokan dengan genre apa saja yang ada dalam suatu data. Secara singkat, data-data genre yang unik dikumpulkan, dilakukan one-hot encoding dan memberikan nilai 1 kepada data yang terdapat dalam list genre.</p>

Mengumpulkan data-data genre yang unik ke dalam satu list
"""

genres=[]
for genre_list in anime.genre:
    genre_list=genre_list.split(", ")
    for genre in genre_list:
        if genre not in genres:
            genres.append(genre)

"""Melakukan one-hot encoding berdasarkan list data-data unik genre yang sebelumnya dikumpulkan. Dataframe dibuat dan diberikan nilai 0 pada setiap data agar jika data terdapat di list genre, kolum genre tersebut akan bernilai 1."""

genre_dict={}
for i in genres:
    genre_dict[i]=[0]
genres=pd.DataFrame(genre_dict,dtype=int)
del(genre_dict)
genres

"""<p>Terdapat 43 variabel genre hasil one-hot encoding</p>
<p>Sekarang, menggabungkan dataframe genre yang telah dilakukan one-hot encoding dengan dataframe anime</p>
"""

anime_joined=pd.concat([anime.anime_id,genres],axis=1)
anime_joined

"""Karena penggabungan,"""

anime_joined=anime_joined.fillna(int(0))
anime_joined=anime_joined.astype('int')
anime_joined

anime_joined.isnull().sum()

anime_joined=pd.merge(anime,anime_joined,on="anime_id")
anime_joined

for i,genres in enumerate(anime_joined.genre):
    for genre in genres.split(", "):
        label=genre
        anime_joined.loc[i,label]=1

anime_joined

anime_joined=pd.concat([anime_joined,pd.get_dummies(anime_joined["type"],prefix="type",dtype=int)],axis=1)
anime_joined

genres_features=anime_joined.columns.to_list()[7:]
genres_features

anime_joined

try:
    anime_joined=anime_joined.drop(["type","rating","members"],axis=1)
except:
    pass
anime_joined

pd.DataFrame(anime_joined[genres_features].to_numpy(),index=anime_joined.name,columns=genres_features)

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(anime_joined[genres_features].to_numpy())

cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=anime_joined.name, columns=anime_joined.name)
print('Shape:', cosine_sim_df.shape)

cosine_sim_df

def anime_recommendations(anime_title, similarity_data=cosine_sim_df, items=anime_joined[anime_joined.columns.to_list()[1:]], k=5):
    """
    Rekomendasi anime berdasarkan kemiripan dataframe

    Parameter:
    ---
    anime_title : tipe data string (str)
                judul anime (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan resto sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,anime_title].to_numpy().argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop anime_title agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(anime_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

anime_joined.loc[anime_joined.name=="Toaru Majutsu no Index"]

anime_recommendations("Toaru Majutsu no Index")

"""<h2>Collaborative Filtering</h2>"""

import pandas as pd
import numpy as np
from zipfile import ZipFile
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""<h3>Data Understanding</h3>"""

df = users

df.info()

df

"""rating -1 berarti penonton hanya menonton dan tidak memberikan rating"""

df = df.loc[df["rating"]!=-1]
df

"""<h3>Data Preparation</h3>
Mengecek apakah dataset memiliki nilai kosong
"""

df.isnull().sum()

"""Dataset tidak memiliki nilai data kosong

Menghilankan data yang anime_id nya tidak ada di dataset anime yang telah diolah
"""

df=df[df.anime_id.isin(anime_joined.anime_id.values)]
df

"""<h4>Encoding user data</h4>
UserId and animeId perlu disandikan (encode) agar mudah terbaca oleh model
"""

userId = df.user_id.unique().tolist()
animeId = anime_joined.anime_id.tolist()

#encoding userId
userIdEncoded = {x:i for i,x in enumerate(userId)}
# print("encoded userId: ",userIdEncoded)

#encoding angka ke userId
encodedToUserId = {i:x for i,x in enumerate(userId)}
# print("encoded angka ke userId: ",encodedToUserId)

#encoding animeId
animeIdEncoded = {x:i for i,x in enumerate(animeId)}
# print("encoded animeid: ",animeIdEncoded)

#encoding angka ke animeId
encodedToAnimeId = {i:x for i,x in enumerate(animeId)}
# print("encoded angka ke animeId: ",encodedToAnimeId)

anime_df = anime[anime.anime_id.isin(df.anime_id.unique())]
anime_df

"""Mapping user_id dan anime_id ke dataframe"""

df.user_id = df.user_id.map(userIdEncoded)
df.anime_id = df.anime_id.map(animeIdEncoded)
df

# Mendapatkan jumlah user
num_users = len(userIdEncoded)


# Mendapatkan jumlah judul anime
num_anime = len(animeIdEncoded)

# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(df['rating'])

# Nilai maksimal rating
max_rating = max(df['rating'])

print('Number of User: {}, Number of anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""Mempersiapkan data training dan test"""

from sklearn.model_selection import train_test_split

#mengacak data
df = df.sample(frac=1, random_state=42)

x = df[["user_id","anime_id"]].values
y = df["rating"].apply(lambda x: (x - min_rating)/(max_rating - min_rating)).values

X_train, X_test, y_train, y_test = train_test_split(x,y,train_size=0.2, random_state=42)
X_train

"""<h2>Training Model</h2>"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( # layer embedding user
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) # layer embedding user bias
    self.anime_embeddings = layers.Embedding( # layer embeddings anime
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1) # layer embedding anime bias

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    anime_vector = self.anime_embeddings(inputs[:, 1]) # memanggil layer embedding 3
    anime_bias = self.anime_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_resto = tf.tensordot(user_vector, anime_vector, 2)

    x = dot_user_resto + user_bias + anime_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_anime, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

import tensorflow as tf
from tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get("root_mean_squared_error")<=0.15):
      print("root_mean_squared_error telah mencapai <=0.15!")
      self.model.stop_training = True
callbacks = myCallback()

reduce_lr = ReduceLROnPlateau(monitor='root_mean_squared_error', factor=0.5, patience=5, min_lr=1e-4, verbose=1)
early_stopping = EarlyStopping(monitor="root_mean_squared_error", mode='min', min_delta=0.1, patience=10, restore_best_weights=True)

# Memulai training

history = model.fit(
    x = X_train,
    y = y_train,
    batch_size = 16,
    epochs = 3,
    validation_data = (X_test, y_test))

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""<h3>Menyimpan Model</h3>"""

model.save('my_model.keras')

user_id = df.user_id.sample(1).iloc[0]
anime_watched_by_user = df[df.user_id == user_id]
anime_not_watched = anime_df[~anime_df["anime_id"].isin(anime_watched_by_user.anime_id.values)]["anime_id"]
# anime_not_watched = [[animeIdEncoded.get(x)] for x in anime_not_watched]
anime_not_watched = list(
    set(anime_not_watched)
    .intersection(set(animeIdEncoded.keys()))
)
anime_not_watched = [[animeIdEncoded.get(x)] for x in anime_not_watched]
user_encoder = userIdEncoded.get(user_id)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_watched),anime_not_watched)
)

ratings = model.predict(user_anime_array).flatten()
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    encodedToAnimeId.get(anime_not_watched[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 9)
print('anime with high ratings from user')
print('----' * 8)

top_anime_user = (
    anime_watched_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)

resto_df_rows = anime_joined[anime_joined['anime_id'].isin(top_anime_user)]
for row in resto_df_rows.itertuples():
    print(row.name, ':', row.genre)

print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)

recommended_anime = anime_joined[anime_joined['anime_id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.name, ':', row.genre)